# ğŸ“¸ AI Image Captioning (SOTA + React)

A modern **full-stack AI application** that generates high-quality, human-like captions for images using **State-of-the-Art (SOTA) deep learning models**.

The system follows a **decoupled architecture**:

- **FastAPI backend** for inference (Dockerized & deployed on Hugging Face Spaces)
- **Next.js / React frontend** for user interaction (deployed on Vercel)

---

## ğŸš€ Live Demo

- **Frontend:** *[https://captioning-hnu.vercel.app/](https://captioning-hnu.vercel.app/)
- **Backend API:** [https://mokh2x-captioning.hf.space/predict](https://mokh2x-captioning.hf.space/predict)

---

## ğŸ—ï¸ Architecture

The project is divided into two independent components:

### ğŸ”¹ Backend â€” Inference Engine

- **Framework:** FastAPI (Python)
- **Models:**
  - BLIP (default)
  - ViT-GPT2
  - Custom ResNet50 + GPT-2
- **Deployment:** Docker container on Hugging Face Spaces (CPU Basic)

### ğŸ”¹ Frontend â€” User Interface

- **Framework:** Next.js (React) + TypeScript
- **Styling:** Tailwind CSS with animations
- **Deployment:** Vercel

---

## ğŸ§  Supported AI Models

### 1ï¸âƒ£ BLIP (Bootstrapping Language-Image Pre-training)

- **Status:** âœ… Default (Best Performance)
- **Description:** Produces highly accurate, detailed, and natural image captions.

### 2ï¸âƒ£ ViT-GPT2

- **Status:** âœ… Available
- **Description:** Combines a Vision Transformer (ViT) encoder with a GPT-2 decoder.

### 3ï¸âƒ£ ResNet50 + GPT-2 (Custom)

- **Status:** ğŸ§ª Experimental / Legacy
- **Description:** Custom implementation trained from scratch on the Flickr30k dataset.

---

## ğŸ› ï¸ Installation & Local Setup

### 1ï¸âƒ£ Backend Setup (Python / Docker)

#### Option A: Run Locally

```bash
# Clone the repository
git clone -b sota https://github.com/Tu2525/MLProject.git
cd MLProject

# Install dependencies
pip install -r requirements.txt

# Run the API
uvicorn src.api.app:app --host 0.0.0.0 --port 8000 --reload
```

Backend will be available at: ğŸ‘‰ [http://localhost:8000](http://localhost:8000)

---

#### Option B: Run with Docker

```bash
# Build Docker image
docker build -t caption-api .

# Run container
docker run -p 7860:7860 caption-api
```

Backend will be available at: ğŸ‘‰ [http://localhost:7860](http://localhost:7860)

---

### 2ï¸âƒ£ Frontend Setup (Next.js)

```bash
# Create a Next.js app
npx create-next-app@latest my-portfolio
cd my-portfolio

# Install dependencies
npm install axios

# Run development server
npm run dev
```

Frontend will be available at: ğŸ‘‰ [http://localhost:3000](http://localhost:3000)

---

## âš™ï¸ Configuration

To change the active model, edit `config/config.py`:

```python
class Config:
    # Options: "blip", "vit_gpt2", "resnet_gpt2"
    MODEL_TYPE = "blip"

    # Force CPU (required for Hugging Face free tier)
    DEVICE = "cpu"
```

---

## ğŸ‘¥ Team

Developed by **Intelligent Systems Engineering students** under the supervision of **Dr. Hadeer Ahmed** at **Helwan National University**:

- Mohammed Mokhtar
- Amr Khaled
- Eyad Ahmed
- Tarek Shereen

---

## ğŸ“„ License

This project is licensed under the **MIT License**.

